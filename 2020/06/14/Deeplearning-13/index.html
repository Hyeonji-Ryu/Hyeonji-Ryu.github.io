<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://hyeonji-ryu.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.">
<meta property="og:type" content="article">
<meta property="og:title" content="13. 인공신경망 최적화 - 드랍아웃(Dropout)">
<meta property="og:url" content="https://hyeonji-ryu.github.io/2020/06/14/Deeplearning-13/index.html">
<meta property="og:site_name" content="AI &amp; ML">
<meta property="og:description" content="해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.">
<meta property="og:locale" content="ko_KR">
<meta property="og:image" content="https://hyeonji-ryu.github.io/images/63.png">
<meta property="og:image" content="https://hyeonji-ryu.github.io/images/64.png">
<meta property="article:published_time" content="2020-06-14T06:07:18.000Z">
<meta property="article:modified_time" content="2020-06-29T10:29:40.682Z">
<meta property="article:author" content="Hyeonji Ryu">
<meta property="article:tag" content="딥러닝">
<meta property="article:tag" content="머신러닝">
<meta property="article:tag" content="Deeplearning">
<meta property="article:tag" content="줄리아">
<meta property="article:tag" content="신경망">
<meta property="article:tag" content="Dropout">
<meta property="article:tag" content="드랍아웃">
<meta property="article:tag" content="MLP">
<meta property="article:tag" content="뉴럴네트워크">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hyeonji-ryu.github.io/images/63.png">

<link rel="canonical" href="https://hyeonji-ryu.github.io/2020/06/14/Deeplearning-13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>13. 인공신경망 최적화 - 드랍아웃(Dropout) | AI & ML</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AI & ML</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Julia, Python, R</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>홈</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>태그</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>카테고리</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>아카이브</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>검색
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Hyeonji-Ryu" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="ko">
    <link itemprop="mainEntityOfPage" href="https://hyeonji-ryu.github.io/2020/06/14/Deeplearning-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hyeonji Ryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AI & ML">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          13. 인공신경망 최적화 - 드랍아웃(Dropout)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">작성일</span>

              <time title="Post created: 2020-06-14 15:07:18" itemprop="dateCreated datePublished" datetime="2020-06-14T15:07:18+09:00">2020-06-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Updated at: 2020-06-29 19:29:40" itemprop="dateModified" datetime="2020-06-29T19:29:40+09:00">2020-06-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning-in-Julia/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning in Julia</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.<br><a id="more"></a></p>
<hr>
<h2 id="드랍아웃-Dropout-이란"><a href="#드랍아웃-Dropout-이란" class="headerlink" title="드랍아웃(Dropout)이란"></a>드랍아웃(Dropout)이란</h2><p>드랍아웃은 인공신경망 훈련 과정을 최적화하기 위한 방법 중 하나이며, 오버피팅(overfitting)을 방지함으로써 모델의 정확도를 높여준다.</p>
<p><strong>Note</strong><br>오버피팅(overfitting)이란?<br>신경망을 학습하는 과정에서 훈련데이터에만 적합한 형태로 학습되는 현상을 오버피팅이라고 한다. 훈련데이터의 정확도는 거의 100%를 달성하는데 실제데이터에서는 일정 이상의 정확도에서 상승하지 않는 것이다. 이런 현상은 보통 훈련데이터를 너무 적게 사용한 경우 또는 모델 파라미터가 너무 많은 경우에 발생한다.</p>
<p>드랍아웃은 각각의 훈련데이터들이 결과값으로 연결되는 신호(엣지, Edge)를 일정한 퍼센트로 삭제함으로써 훈련데이터의 일부만 파라미터에 영향을 줄 수 있도록 조정하는 역할을 한다. 드랍아웃을 함수로 만들면 다음과 같다.</p>
<h2 id="드랍아웃-구현"><a href="#드랍아웃-구현" class="headerlink" title="드랍아웃 구현"></a>드랍아웃 구현</h2><p>드랍아웃은 입력된 비율에 따라 몇몇의 신호값들을 0으로 반환한다. 이를 함수로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> Random</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> drop_out_single(input_size, rate)</span><br><span class="line">    <span class="keyword">function</span> changing_T_or_F_with_percentage(number, input_size, rate)</span><br><span class="line">        temp_num = input_size * rate</span><br><span class="line">        <span class="keyword">if</span> number &gt; temp_num</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    temp = shuffle(reshape(<span class="number">1</span>:input_size, <span class="number">1</span>, input_size))</span><br><span class="line">    <span class="keyword">return</span> changing_T_or_F_with_percentage.(temp, input_size, rate)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>위 함수는 신호값과 곱하는 마스크를 생성한다. 드랍아웃에서는 비율을 입력값으로 받아 입력된 신호값의 일부 위치를 무작위로 선정하고, 그 자리의 신호를 0으로 반환해야 하는데 이를 구현하기 위해 기술적으로 신호값과 곱해주는 마스크를 생성하는 것이다. 하지만 이는 데이터 하나의 형태(한 줄)에만 작동하는 함수이다. 우리는 지금까지 배치데이터(여러 줄)를 사용해왔기 때문에 각각의 모든 줄에 위의 드랍아웃을 적용하는 함수가 필요하다.</p>
<p><strong>Note</strong><br>배치데이터에서 드랍아웃의 작동원리<br>드랍아웃은 각각의 데이터들이 동일한 비율로 신호값이 제거되어야 한다. 다시 말해, 한 줄로 나열된 데이터를 여러 개 합쳐놓은 매트릭스 형태의 배치데이터에서는 한 줄마다 일정한 비율을 유지해주면서 신호값을 제거해야 한다. 그렇기에 한 줄씩 인덱스를 무작위로 선정하여 제거해주는 작업이 필수적이다. 만약 이를 고려하지 않고 배치데이터를 드랍아웃한다면, 이는 효과가 거의 없다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> drop_out(input_size, hidden_size, rate)</span><br><span class="line">    temp = drop_out_single(hidden_size, rate)</span><br><span class="line">    temp_num = input_size - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i = <span class="number">1</span>:temp_num</span><br><span class="line">        temp_1 = drop_out_single(hidden_size, rate)</span><br><span class="line">        temp = [temp; temp_1]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> temp</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>위 함수는 <code>drop_out_single()</code>을 배치데이터에 사용할 수 있도록 변환한 것이다. 위 함수에서 사용된 <code>input_size</code>와 <code>hidden_size</code>는 배치데이터에서 행렬의 형상이다. 예를 들어 입력된 신호값이 $10 \times 784$의 행렬이라면 이는 $1 \times 784$ 데이터가 총 10개가 포함된 배치데이터이기에 <code>drop_out_single()</code>을 <code>input_size</code>만큼 반복하는 것이다.</p>
<p>신호값과 곱해줄 드랍아웃 마스크는 완성되었다. 이제는 신호값과 마스크를 곱해주는 함수를 생성해보자. 참고로 드랍아웃도 신호값을 제거하는 과정이기에 이후 역전파에서 같은 위치의 미분값이 제거되어야 한다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">mutable struct</span> Dropout</span><br><span class="line">    mask</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> dropout_forward(dropout, x, dropout_ratio)</span><br><span class="line">    <span class="keyword">if</span> dropout_ratio &lt; <span class="number">1</span></span><br><span class="line">        dropout.mask = drop_out(size(x)[<span class="number">1</span>],size(x)[<span class="number">2</span>], dropout_ratio)</span><br><span class="line">        <span class="keyword">return</span> x .* dropout.mask</span><br><span class="line">    <span class="keyword">else</span> dropout_ratio = <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> x .*  (<span class="number">1.0</span> - dropout_ratio)</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> dropout_backward(dropout, dout)</span><br><span class="line">    <span class="keyword">return</span> dout .* dropout.mask</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>이제 드랍아웃을 위한 함수 구현은 모두 끝났다. 다음으로는 위 함수들을 이용하여 드랍아웃을 적용한 모델과 적용하지 않은 모델을 비교해보자.</p>
<h2 id="신경망-모델-구현"><a href="#신경망-모델-구현" class="headerlink" title="신경망 모델 구현"></a>신경망 모델 구현</h2><p>신경망 모델은 지금까지 구현했던 <code>MNIST</code>데이터를 사용하는 2층 신경망 모델을 다시 사용할 것이다. 역전파 모델에 대한 정보는 <a href="https://hyeonji-ryu.github.io/2020/04/20/Deeplearning-9/">해당 글</a>에서 확인할 수 있다. 또한 모델 구성에 필요한 함수들은 <a href="https://github.com/Hyeonji-Ryu/Machine_Learning_in_Julia/blob/master/MLP/backward_propagation.jl" target="_blank" rel="noopener">깃허브</a>에서 찾아볼 수 있다. 준비가 완료되었다면 본격적으로 구현해보자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#훈련데이터 300개</span></span><br><span class="line"></span><br><span class="line">train_x = train_x[<span class="number">1</span>:<span class="number">300</span>,:]</span><br><span class="line">train_y = train_y[<span class="number">1</span>:<span class="number">300</span>,:]</span><br><span class="line">t = t[<span class="number">1</span>:<span class="number">300</span>,:]</span><br></pre></td></tr></table></figure>
<p>이번 구현에서는 오버피팅을 발생시키기 위해서 60000개인 <code>train_x</code>데이터 중에서 300개만을 사용하여 학습시킬 것이다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">Dict</span>()</span><br><span class="line">grads = <span class="built_in">Dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 층에 들어갈 가중치와 편향 입력</span></span><br><span class="line">W = [<span class="string">"W1"</span>, <span class="string">"W2"</span>]</span><br><span class="line">b = [<span class="string">"b1"</span>, <span class="string">"b2"</span>]</span><br><span class="line">hidden_size = [<span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">making_network(W,b,<span class="number">784</span>,hidden_size,<span class="number">10</span>,<span class="string">"std"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 계층마다 인스턴스를 만들어줘야 한다. (for 역전파)</span></span><br><span class="line"></span><br><span class="line">result = SoftmaxwithLoss(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">dense1 = dense_layer(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">dense2 = dense_layer(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">Sigmoid1 = Sigmoid(<span class="number">0</span>)</span><br><span class="line">Relu = ReLu(<span class="number">0</span>)</span><br><span class="line">optimizer = optimizers(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">accuracy_test = <span class="built_in">Float64</span>[]</span><br><span class="line">accuracy_train = <span class="built_in">Float64</span>[]</span><br><span class="line">train_size = size(train_x)[<span class="number">1</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br></pre></td></tr></table></figure>
<p>모델 학습을 위한 변수들을 정의한다. 참고로 여기서 사용된 <code>making_network()</code>는 이전글인 <a href="https://hyeonji-ryu.github.io/2020/05/15/Deeplearning-12/">가중치 초기값</a>에서 사용했던 함수와 다르다. 역전파 알고리즘에서 사용했던 함수와 동일하다. 만약 새로운 <code>making_network()</code>를 사용하고 싶다면, 가중치와 편향 입력 부분을 아래와 같이 변경하면 된다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 층에 들어갈 가중치와 편향 입력</span></span><br><span class="line">W = [<span class="string">"W1"</span>, <span class="string">"W2"</span>]</span><br><span class="line">b = [<span class="string">"b1"</span>, <span class="string">"b2"</span>]</span><br><span class="line">input_size = (<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">hidden_size = [(<span class="number">784</span>,<span class="number">50</span>),(<span class="number">50</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">params = making_network(W, b, weight_size, input_size, <span class="string">"std"</span>)</span><br></pre></td></tr></table></figure>
<p>이제 모델을 작동시킬 준비가 끝났다. 아래의 코드를 입력하여 학습을 시작하자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@time</span> <span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">6000</span></span><br><span class="line">        batch_mask = rand(<span class="number">1</span>:<span class="number">300</span>, <span class="number">100</span>)</span><br><span class="line">        x_batch = train_x[batch_mask, :]</span><br><span class="line">        t_batch = t[batch_mask, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 순전파</span></span><br><span class="line">        z1 = dense_layer_forward(dense1,x_batch,params[<span class="string">"W1"</span>],params[<span class="string">"b1"</span>])</span><br><span class="line">        a1 = relu_forward(Relu,z1)</span><br><span class="line">        z2 = dense_layer_forward(dense2,a1,params[<span class="string">"W2"</span>],params[<span class="string">"b2"</span>])</span><br><span class="line">        num = SoftmaxwithLoss_forward(z2,t_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 역전파</span></span><br><span class="line">        last_layer = SoftmaxwithLoss_backward(result)</span><br><span class="line">        z2_back = dense_layer_backward(dense2, last_layer)</span><br><span class="line">        grads[<span class="string">"W2"</span>] = dense2.dw</span><br><span class="line">        grads[<span class="string">"b2"</span>] = dense2.db</span><br><span class="line">        a1_back = relu_backward(Relu,z2_back)</span><br><span class="line">        z1_back = dense_layer_backward(dense1, a1_back)</span><br><span class="line">        grads[<span class="string">"W1"</span>] = dense1.dw</span><br><span class="line">        grads[<span class="string">"b1"</span>] = dense1.db</span><br><span class="line"></span><br><span class="line">        <span class="comment">#가중치 갱신</span></span><br><span class="line">        SGD(params, grads)</span><br><span class="line"></span><br><span class="line">        temp_loss = loss(x_batch, t_batch)</span><br><span class="line">        print(<span class="string">"NO.<span class="variable">$i</span>: "</span>)</span><br><span class="line">        println(temp_loss)</span><br><span class="line">        append!(train_loss_list, temp_loss)</span><br><span class="line">        append!(accuracy_test, evaluate(test_x, test_y))</span><br><span class="line">        append!(accuracy_train, evaluate(train_x, train_y))</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>위 모델은 300개의 데이터를 100개 배치데이터 단위로 2000에폭 학습한다. 이제 위의 결과를 그래프를 그려 확인해보자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">without_train = vcat(accuracy_train)</span><br><span class="line">without_test = vcat(accuracy_test)</span><br><span class="line"></span><br><span class="line">x = range(<span class="number">1</span>,length(without_train),step=<span class="number">1</span>)</span><br><span class="line">data = [without_train without_test]</span><br><span class="line">labels = [<span class="string">"accuracy_train"</span> <span class="string">"accuracy_test"</span>]</span><br><span class="line">markercolors = [</span><br><span class="line">    :red :blue</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">pl_nodrop=plot(</span><br><span class="line">    x,</span><br><span class="line">    data,</span><br><span class="line">    label = labels,</span><br><span class="line">    color = markercolors,</span><br><span class="line">    markersize = <span class="number">4</span>,</span><br><span class="line">    title = <span class="string">"Accuracy without Dropout"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>그 결과는 아래와 같다.</p>
<p><img src="\../images/63.png" alt="드랍아웃 없음"></p>
<p>그래프를 확인해보면 훈련데이터는 정확도가 거의 100%에 가깝지만 실제데이터의 정확도는 80% 근방에서 멈춘 것을 확인할 수 있다. 따라서 이런 경우 드랍아웃을 추가하면 위의 현상을 완화시킬 수 있다. 이제 드랍아웃이 적용된 모델을 확인하자.</p>
<p><strong>WARNING</strong><br>모델을 다시 훈련시키기에 앞서 가중치와 편향을 다시 초기화해주어야 한다. 따라서 다시 아래의 코드를 작동시켜야 한다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">Dict</span>()</span><br><span class="line">grads = <span class="built_in">Dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 층에 들어갈 가중치와 편향 입력</span></span><br><span class="line">W = [<span class="string">"W1"</span>, <span class="string">"W2"</span>]</span><br><span class="line">b = [<span class="string">"b1"</span>, <span class="string">"b2"</span>]</span><br><span class="line">hidden_size = [<span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">making_network(W,b,<span class="number">784</span>,hidden_size,<span class="number">10</span>,<span class="string">"std"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 계층마다 인스턴스를 만들어줘야 한다. (for 역전파)</span></span><br><span class="line"></span><br><span class="line">result = SoftmaxwithLoss(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">dense1 = dense_layer(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">dense2 = dense_layer(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">Sigmoid1 = Sigmoid(<span class="number">0</span>)</span><br><span class="line">Relu = ReLu(<span class="number">0</span>)</span><br><span class="line">optimizer = optimizers(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">accuracy_test = <span class="built_in">Float64</span>[]</span><br><span class="line">accuracy_train = <span class="built_in">Float64</span>[]</span><br><span class="line">train_size = size(train_x)[<span class="number">1</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br></pre></td></tr></table></figure>
<p>이제 모델을 학습해보자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@time</span> <span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">6000</span></span><br><span class="line">        batch_mask = rand(<span class="number">1</span>:<span class="number">300</span>, <span class="number">100</span>)</span><br><span class="line">        x_batch = train_x[batch_mask, :]</span><br><span class="line">        t_batch = t[batch_mask, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 신경망 계산</span></span><br><span class="line">        z1 = dense_layer_forward(dense1,x_batch,params[<span class="string">"W1"</span>],params[<span class="string">"b1"</span>])</span><br><span class="line">        a1 = relu_forward(Relu,z1)</span><br><span class="line">        dt = dropout_forward(dropout, a1, <span class="number">0.3</span>) <span class="comment"># 드랍아웃 레이어</span></span><br><span class="line">        z2 = dense_layer_forward(dense2,dt,params[<span class="string">"W2"</span>],params[<span class="string">"b2"</span>])</span><br><span class="line">        num = SoftmaxwithLoss_forward(z2,t_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 역전파</span></span><br><span class="line">        last_layer = SoftmaxwithLoss_backward(result)</span><br><span class="line">        z2_back = dense_layer_backward(dense2, last_layer)</span><br><span class="line">        grads[<span class="string">"W2"</span>] = dense2.dw</span><br><span class="line">        grads[<span class="string">"b2"</span>] = dense2.db</span><br><span class="line">        dt_back = dropout_backward(dropout,z2_back) <span class="comment"># 드랍아웃 레이어</span></span><br><span class="line">        a1_back = relu_backward(Relu, dt_back)</span><br><span class="line">        z1_back = dense_layer_backward(dense1, a1_back)</span><br><span class="line">        grads[<span class="string">"W1"</span>] = dense1.dw</span><br><span class="line">        grads[<span class="string">"b1"</span>] = dense1.db</span><br><span class="line"></span><br><span class="line">        <span class="comment">#가중치 갱신</span></span><br><span class="line">        SGD(params, grads)</span><br><span class="line"></span><br><span class="line">        temp_loss = loss(x_batch, t_batch)</span><br><span class="line">        print(<span class="string">"NO.<span class="variable">$i</span>: "</span>)</span><br><span class="line">        println(temp_loss)</span><br><span class="line">        append!(train_loss_list, temp_loss)</span><br><span class="line">        append!(accuracy_test, evaluate(test_x, test_y))</span><br><span class="line">        append!(accuracy_train, evaluate(train_x, train_y))</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>위 모델은 중간에 드랍아웃이 적용되어 있다. 드랍아웃 레이어를 확인해보면 비율 파라미터 자리에 <code>0.3</code>이 있다. 즉, 30%의 신호값을 제거하라는 의미이다. 이제 위의 결과를 그래프를 그려 확인해보자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">with_train = vcat(accuracy_train)</span><br><span class="line">with_test = vcat(accuracy_test)</span><br><span class="line"></span><br><span class="line">x = range(<span class="number">1</span>,length(with_train),step=<span class="number">1</span>)</span><br><span class="line">data = [with_train with_test]</span><br><span class="line">labels = [<span class="string">"accuracy_train"</span> <span class="string">"accuracy_test"</span>]</span><br><span class="line">markercolors = [</span><br><span class="line">    :red :blue</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">pl_drop=plot(</span><br><span class="line">    x,</span><br><span class="line">    data,</span><br><span class="line">    label = labels,</span><br><span class="line">    color = markercolors,</span><br><span class="line">    markersize = <span class="number">4</span>,</span><br><span class="line">    title = <span class="string">"Accuracy with Dropout"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>결과는 다음과 같다.</p>
<p><img src="\../images/64.png" alt="드랍아웃 있음"></p>
<p>훈련데이터의 정확도와 실제데이터의 정확도 간격이 드랍아웃을 적용하지 않은 모델보다 훨씬 줄어든 것을 확인할 수 있다.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/" rel="tag"># 딥러닝</a>
              <a href="/tags/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/" rel="tag"># 머신러닝</a>
              <a href="/tags/Deeplearning/" rel="tag"># Deeplearning</a>
              <a href="/tags/%EC%A4%84%EB%A6%AC%EC%95%84/" rel="tag"># 줄리아</a>
              <a href="/tags/%EC%8B%A0%EA%B2%BD%EB%A7%9D/" rel="tag"># 신경망</a>
              <a href="/tags/Dropout/" rel="tag"># Dropout</a>
              <a href="/tags/%EB%93%9C%EB%9E%8D%EC%95%84%EC%9B%83/" rel="tag"># 드랍아웃</a>
              <a href="/tags/MLP/" rel="tag"># MLP</a>
              <a href="/tags/%EB%89%B4%EB%9F%B4%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/" rel="tag"># 뉴럴네트워크</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/15/Deeplearning-12/" rel="prev" title="12. 인공신경망 최적화 - 가중치 초기값">
      <i class="fa fa-chevron-left"></i> 12. 인공신경망 최적화 - 가중치 초기값
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/20/Deeplearning-14/" rel="next" title="14. CNN 시작하기">
      14. CNN 시작하기 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          목차
        </li>
        <li class="sidebar-nav-overview">
          훑어보기
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#드랍아웃-Dropout-이란"><span class="nav-number">1.</span> <span class="nav-text">드랍아웃(Dropout)이란</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#드랍아웃-구현"><span class="nav-number">2.</span> <span class="nav-text">드랍아웃 구현</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#신경망-모델-구현"><span class="nav-number">3.</span> <span class="nav-text">신경망 모델 구현</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hyeonji Ryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">포스트</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">카테고리</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">83</span>
        <span class="site-state-item-name">태그</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hyeonji Ryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
