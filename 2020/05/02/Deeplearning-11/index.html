<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://hyeonji-ryu.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.">
<meta property="og:type" content="article">
<meta property="og:title" content="11. 인공신경망 최적화 - Optimizer">
<meta property="og:url" content="https://hyeonji-ryu.github.io/2020/05/02/Deeplearning-11/index.html">
<meta property="og:site_name" content="Dev anything">
<meta property="og:description" content="해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.">
<meta property="og:locale" content="ko_KR">
<meta property="og:image" content="https://hyeonji-ryu.github.io/images/56.png">
<meta property="og:image" content="https://hyeonji-ryu.github.io/images/57.png">
<meta property="article:published_time" content="2020-05-02T10:51:05.000Z">
<meta property="article:modified_time" content="2020-06-08T04:10:33.467Z">
<meta property="article:author" content="Hyeonji Ryu">
<meta property="article:tag" content="딥러닝">
<meta property="article:tag" content="Deeplearning">
<meta property="article:tag" content="머신러닝">
<meta property="article:tag" content="줄리아">
<meta property="article:tag" content="신경망">
<meta property="article:tag" content="optimizer">
<meta property="article:tag" content="옵티마이저">
<meta property="article:tag" content="Adam">
<meta property="article:tag" content="AdaGrad">
<meta property="article:tag" content="SGD">
<meta property="article:tag" content="Momentum">
<meta property="article:tag" content="RMSProp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hyeonji-ryu.github.io/images/56.png">

<link rel="canonical" href="https://hyeonji-ryu.github.io/2020/05/02/Deeplearning-11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>11. 인공신경망 최적화 - Optimizer | Dev anything</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163411677-2"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-163411677-2');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dev anything</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Welcome :)</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>홈</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>태그</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>카테고리</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>아카이브</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>검색
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Hyeonji-Ryu" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="ko">
    <link itemprop="mainEntityOfPage" href="https://hyeonji-ryu.github.io/2020/05/02/Deeplearning-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hyeonji Ryu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dev anything">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          11. 인공신경망 최적화 - Optimizer
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">작성일</span>

              <time title="Post created: 2020-05-02 19:51:05" itemprop="dateCreated datePublished" datetime="2020-05-02T19:51:05+09:00">2020-05-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Updated at: 2020-06-08 13:10:33" itemprop="dateModified" datetime="2020-06-08T13:10:33+09:00">2020-06-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning-in-Julia/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning in Julia</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>해당 시리즈는 프로그래밍 언어 중 하나인 줄리아(Julia)로 딥러닝(Deep learning)을 구현하면서 원리를 설명합니다.<br><a id="more"></a></p>
<hr>
<h2 id="인공신경망-최적화란"><a href="#인공신경망-최적화란" class="headerlink" title="인공신경망 최적화란"></a>인공신경망 최적화란</h2><p>무작위값으로 가중치와 편향이 주어진 인공신경망은 바보와 같다. 따라서 신경망은 학습을 통해서 데이터에 적절한 가중치와 편향 값을 찾아야 한다. <strong>인공신경망 최적화란 데이터에 따라 더 적절한 학습 방법을 찾는 과정을 의미한다.</strong> 즉, 적절한 매개 변수 값을 찾기 위한 학습을 좀 더 효율적으로 하고 싶은 사람들이 만든 기술이라는 것이다. 이번 글부터는 최적화에 대해서 다룰 것이며, 어떤 부분들을 최적화할 수 있는지 알아보자.</p>
<p>신경망 학습을 최적화하기 위해서는 몇 가지의 설정이 필요하다. 필요한 설정은 다음과 같다.</p>
<ul>
<li>미분을 어떻게 구할 것인가? (순전파 또는 역전파)</li>
<li>매개 변수 갱신을 어떻게 할 것인가? (optimizers)</li>
<li>가중치의 초기값을 어떻게 설정할 것인가? (std, Xavier, He)</li>
<li>오버피팅을 어떻게 막을 것인가? (가중치 감소 또는 Dropout)</li>
<li>배치 정규화를 사용할 것인가?</li>
</ul>
<p>위의 질문 중에서 첫 번째는 이전 글들에서 직접 확인하였다. 따라서 첫 번째 질문에 대한 답변은 <a href="https://hyeonji-ryu.github.io/2020/04/04/Deeplearning-7/">순전파 알고리즘</a>과 <a href="https://hyeonji-ryu.github.io/2020/04/20/Deeplearning-9/">역전파 알고리즘</a>으로 대신할 것이다.</p>
<p>다음 질문은 매개 변수 갱신 방법에 대한 논의이며, 이번 글에서 우리가 다룰 주제이다.</p>
<h2 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h2><p>옵티마이저(Optimizer)는 도출된 미분 값을 어떻게 계산하여 적용하는지를 다루는 기법이다. 종류는 매우 다양하지만 이 글에서는 ‘SGD’, ‘Momentum’, ‘AdaGrad’, ‘RMSProp’, ‘Adam’만 다룰 것이다. 몇몇 옵티마이저는 갱신할 때 이전 값들을 필요로 한다. 따라서 옵티마이저 구조체를 설정하여 값을 보관해야 한다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">mutable struct</span> optimizers</span><br><span class="line">    v</span><br><span class="line">    h</span><br><span class="line">    m</span><br><span class="line">    iter</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">optimizer = optimizers(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>SGD는 ‘Stochastic Gradient Descent’의 약자로서 한국어로 ‘확률적 경사 하강법’이라고도 불린다. SGD 수식은 다음과 같다.</p>
<script type="math/tex; mode=display">W \leftarrow W - \eta \frac{\partial L}{\partial W}</script><p>위 식을 분석해보면 SGD는 매개 변수의 편미분 값에 학습률 $\eta$를 곱한 뒤, 기존 매개 변수에서 뺀다. 계산된 결과를 다시 매개 변수로 갱신한다.</p>
<p>위의 식을 코드로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> SGD(params,grads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">        params[key] -= learning_rate * grads[key]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><code>SGD()</code>는 기존 매개 변수가 있는 <code>params</code>와 편미분값이 저장된 <code>grads</code>를 인수로 받는다. 이후 위 수식처럼 계산을 하여 다시 <code>params</code>를 갱신한 후 결과로 내보낸다.</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum(이하 모멘텀)은 운동량을 뜻하는 단어로 물리에서 사용하는 원리가 추가되었다. 보통 물리에서 모멘텀은 어떤 물체가 한 방향으로 지속적으로 변동하려는 경향을 뜻한다. 여기서도 같은 맥락으로 사용되었다. 모멘텀의 수식은 다음과 같다.</p>
<script type="math/tex; mode=display">v \leftarrow \alpha v - \eta \frac{\partial L}{\partial W}</script><script type="math/tex; mode=display">W \leftarrow W + v</script><p>위 수식에서 확인할 수 있듯이 모멘텀은 SGD와는 다르게 새로운 변수 $v$가 추가되었다. $v$는 물리에서 속도와 같은 의미이다. 즉, 모멘텀이 한 방향으로 지속하려는 경향을 숫자로 나타낸 것이 $v$인 것이다. 따라서 기울기의 각도가 수직적일수록 속도는 증가하며, 수평적일수록 속도는 감소한다.</p>
<p>모멘텀은 $v$로 인해 로컬 미니멈에 도착하여도 그 공간을 벗어날 수 있다. 이는 로컬 미니멈에 도착하면 기울기가 0이 되어 멈추는 SGD의 단점을 보완한 기법이기도 하다.</p>
<p>모멘텀을 코드로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> Momentum(params,grads)</span><br><span class="line"></span><br><span class="line">    momentum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> optimizer.v == <span class="number">0</span></span><br><span class="line">        optimizer.v = <span class="built_in">Dict</span>()</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">            optimizer.v[key] = zeros(size(params[key]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">        optimizer.v[key] = (optimizer.v[key].* momentum) - (learning_rate * grads[key])</span><br><span class="line">        params[key] += optimizer.v[key]</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><code>Momentum()</code>도 SGD와 같이 <code>params</code>와  <code>grads</code>를 인수로 받은 후 계산하여 <code>params</code>를 갱신한 후 결과로 내보낸다.</p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>매개 변수를 갱신함에 있어서 가장 중요한 부분 중 하나는 ‘학습률’이다. SGD나 모멘텀에서는 학습률 $\eta$가 상수로 사용되었다. 하지만 AdaGrad에서는 학습률이 변수로서 사용되며, 최솟점에 다다를수록 학습률이 감소한다. AdaGrad를 수식으로 나타내면 다음과 같다.</p>
<script type="math/tex; mode=display">h \leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}</script><script type="math/tex; mode=display">W \leftarrow W - \eta \frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}</script><p>위 수식에서 $\odot$는 ‘아다마르 곱(Hadamard product)’이다. 아다마르 곱은 동일한 크기의 행렬 두 개를 원소별로 곱한다. 따라서 첫 번째 수식은 해당 매개 변수의 미분값을 원소 별로 곱셈한 후, 기존의 $h$와 더하여 $h$를 갱신한다. 이렇게 계산된 $h$는 매개 변수를 갱신하는 두 번째 수식으로 들어가며, $\frac{1}{\sqrt{h}}$는 $h$가 클수록 작아진다. 따라서 학습이 진행되면서 $h$는 점점 커지고, 학습률은 점점 작아진다.</p>
<p>AdaGrad를 코드로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> AdaGrad(params,grads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> optimizer.h == <span class="number">0</span></span><br><span class="line">        optimizer.h = <span class="built_in">Dict</span>()</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">            optimizer.h[key] = zeros(size(params[key]))</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">        optimizer.h[key] +=  grads[key] .* grads[key]</span><br><span class="line">        params[key] -= (learning_rate * grads[key]) ./ (optimizer.h[key].^(<span class="number">1</span>/<span class="number">2</span>).+ <span class="number">1e-7</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p> RMSProp는 AdaGrad의 단점을 보완한 기법이다. AdaGrad는 변수 $h$가 지속적으로 증가하면서 이동 속도를 줄이며, 단순한 볼록 형태를 가진 환경에서는 최소점을 잘 찾을 수 있다. 하지만 로컬 미니멈이 존재하는 비볼록 형태의 환경에서는 로컬 미니멈에서 벗어나기 어렵다. 이런 부분을 보완하고자 감소하는 속도를 나타내는 새로운 상수 $\beta$를 대입하여 로컬 미니멈에서 벗어날 수 있도록 수정한 것이 RMSProp 기법이다. 이를 수식으로 보면 다음과 같다.</p>
<script type="math/tex; mode=display">h \leftarrow \beta * h + (1-\beta)(\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W})</script><script type="math/tex; mode=display">W \leftarrow W - \eta \frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}</script><p>위 수식에서 확인할 수 있듯이 감소 속도인 $\beta$가 추가된 것 외에는 AdaGrad와 동일하다. 보통 $\beta$는 0.9를 사용한다. 이를 코드로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> RMSProp(params,grads)</span><br><span class="line"></span><br><span class="line">    beta = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> optimizer.h == <span class="number">0</span></span><br><span class="line">        optimizer.h = <span class="built_in">Dict</span>()</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">            optimizer.h[key] = zeros(size(params[key]))</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">        optimizer.h[key] = (beta * optimizer.h[key]) + (<span class="number">1.0</span> - beta)*(grads[key] .* grads[key])</span><br><span class="line">        params[key] -= (learning_rate * grads[key]) ./ (optimizer.h[key].^(<span class="number">1</span>/<span class="number">2</span>).+ <span class="number">1e-7</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam은 ‘Adaptive moments’의 약자로 모멘텀과 RMSProp를 섞은 형태의 기법이다. Adam에는 총 2개의 변수가 등장하는데, 모멘텀의 속도 원리가 적용된 변수 $m$과 RMSProp에서 감소 속도 원리가 적용된 변수 $v$이다. 또한 RMSProp에서 사용했던 감소 속도 상수인 $\beta$도 사용된다. 보통 $m$에서는 $\beta_1$이 사용되며 $v$에서는 $\beta_2$가 사용된다. 수식은 다음과 같다.</p>
<script type="math/tex; mode=display">m \leftarrow \beta_1 * m + (1-\beta_1)\frac{\partial L}{\partial W}</script><script type="math/tex; mode=display">v \leftarrow \beta_2 * v + (1-\beta_2)(\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W})</script><script type="math/tex; mode=display">\hat{m} \leftarrow \frac{m}{1-\beta_1^t}</script><script type="math/tex; mode=display">\hat{v} \leftarrow \frac{v}{1-\beta_2^t}</script><script type="math/tex; mode=display">W \leftarrow W - \frac{\hat{m}*\eta}{\sqrt{\hat{v}}}</script><p>보통 $\beta_1$은 0.9를 사용하고 $\beta_2$는 0.999를 사용한다. Adam은 학습률로 보통 0.01을 쓰는 다른 기법과 다르게 학습률을 0.001을 사용한다. 이를 코드로 구현하면 다음과 같다.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> Adam(params,grads,learning_rate = <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    beta1 = <span class="number">0.9</span></span><br><span class="line">    beta2 = <span class="number">0.999</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> optimizer.m == <span class="number">0</span></span><br><span class="line">        optimizer.m = <span class="built_in">Dict</span>()</span><br><span class="line">        optimizer.v = <span class="built_in">Dict</span>()</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">            optimizer.m[key] = zeros(size(params[key]))</span><br><span class="line">            optimizer.v[key] = zeros(size(params[key]))</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    optimizer.iter += <span class="number">1</span></span><br><span class="line">    lr_t = learning_rate * (<span class="number">1.0</span> - beta2^(optimizer.iter))^(<span class="number">1</span>/<span class="number">2</span>) / (<span class="number">1.0</span> - beta1^(optimizer.iter))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> keys(params)</span><br><span class="line">        optimizer.m[key] += (<span class="number">1</span> - beta1) * (grads[key] - optimizer.m[key])</span><br><span class="line">        optimizer.v[key] += (<span class="number">1</span> - beta2) * (grads[key].^<span class="number">2</span> - optimizer.v[key])</span><br><span class="line">        params[key] -= (lr_t * optimizer.m[key]) ./ ((optimizer.v[key]).^(<span class="number">1</span>/<span class="number">2</span>) .+ <span class="number">1e-7</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h2 id="옵티마이저-비교"><a href="#옵티마이저-비교" class="headerlink" title="옵티마이저 비교"></a>옵티마이저 비교</h2><p>지금까지 총 5가지의 옵티마이저 기법들을 살펴보았다. 이제 <code>MNIST</code>데이터를 사용하여 각 기법들을 비교해보자.</p>
<p>비교하기 위해 사용할 신경망은 2층 구조이며, 활성화 함수로는 ReLU, 미분은 역전파 알고리즘을 사용할 것이다. 각 옵티마이저 별로 3에폭씩 학습하였다. 우리가 비교할 부분은 크게 3가지인 시간, 정확도, 손실 함수이다. 시간은 적게 걸릴 수록, 정확도는 높을 수록, 손실 함수는 낮을 수록 더 좋은 알고리즘이다.</p>
<p><strong>WARNING</strong><br>이 글에서 진행하는 성능 비교는 2층 신경망이며, MNIST데이터를 기반으로 한 분류 모델이다. 다른 모델에서 성능은 이 글의 결과와 차이가 있을 수 있다.</p>
<p>먼저 5가지의 시간부터 확인해보자.</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SGD: <span class="number">46.004452</span> seconds</span><br><span class="line">Momentum: <span class="number">47.702307</span> seconds</span><br><span class="line">AdaGrad: <span class="number">49.991280</span> seconds</span><br><span class="line">RMSProp: <span class="number">49.674666</span> seconds</span><br><span class="line">Adam: <span class="number">50.698803</span> seconds</span><br></pre></td></tr></table></figure>
<p>대부분 45초~50초 사이의 결과가 나왔다. 각 기법 별로 살펴보면 SGD가 가장 시간이 적게 걸리고 Adam이 가장 오래걸리는 것을 확인할 수 있다. 수식과 시간을 대조하여 확인해보면 수식이 복잡할수록 시간이 오래 걸리는 것을 확인할 수 있다.</p>
<p>다음으로는 정확도를 나타낸 그래프이다.</p>
<p><img src="\../images/56.png" alt="옵티마이저 정확도"></p>
<p>정확도에서는 SGD만 유일하게 90%를 넘지 못하였다. 또한 RMSProp가 다른 기법보다 더 높은 정확도를 도출하는 것을 확인할 수 있다.</p>
<p>다음으로는 손실 함수를 나타낸 그래프를 살펴보자.</p>
<p><img src="\../images/57.png" alt="옵티마이저 손실 함수"></p>
<p>손실 함수 값 또한 RMSProp가 더 낮은 결과를 도출하는 것을 확인할 수 있다.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/" rel="tag"># 딥러닝</a>
              <a href="/tags/Deeplearning/" rel="tag"># Deeplearning</a>
              <a href="/tags/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/" rel="tag"># 머신러닝</a>
              <a href="/tags/%EC%A4%84%EB%A6%AC%EC%95%84/" rel="tag"># 줄리아</a>
              <a href="/tags/%EC%8B%A0%EA%B2%BD%EB%A7%9D/" rel="tag"># 신경망</a>
              <a href="/tags/optimizer/" rel="tag"># optimizer</a>
              <a href="/tags/%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80/" rel="tag"># 옵티마이저</a>
              <a href="/tags/Adam/" rel="tag"># Adam</a>
              <a href="/tags/AdaGrad/" rel="tag"># AdaGrad</a>
              <a href="/tags/SGD/" rel="tag"># SGD</a>
              <a href="/tags/Momentum/" rel="tag"># Momentum</a>
              <a href="/tags/RMSProp/" rel="tag"># RMSProp</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/02/Deeplearning-10/" rel="prev" title="10. ReLU vs. Sigmoid 성능 비교">
      <i class="fa fa-chevron-left"></i> 10. ReLU vs. Sigmoid 성능 비교
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/14/mathematic-3/" rel="next" title="1. Linear Algebra (3)">
      1. Linear Algebra (3) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          목차
        </li>
        <li class="sidebar-nav-overview">
          훑어보기
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#인공신경망-최적화란"><span class="nav-number">1.</span> <span class="nav-text">인공신경망 최적화란</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizers"><span class="nav-number">2.</span> <span class="nav-text">Optimizers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SGD"><span class="nav-number">2.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Momentum"><span class="nav-number">2.2.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaGrad"><span class="nav-number">2.3.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSProp"><span class="nav-number">2.4.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">2.5.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#옵티마이저-비교"><span class="nav-number">3.</span> <span class="nav-text">옵티마이저 비교</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hyeonji Ryu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">69</span>
          <span class="site-state-item-name">포스트</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">카테고리</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">132</span>
        <span class="site-state-item-name">태그</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hyeonji Ryu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
